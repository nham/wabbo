---
title: Markov decision processes
---

This is not the broadest possible definition of a Markov decision process, but we'll stick with it for now in order to have something simple to grasp. It can be generalized later.

A **Markov decision process** includes the following mathematical objects:

 - a set of decision instants $T$, which we specify in more detail below
 - a countable state space $S$
 - a finite action space $A$
 - a bounded reward function $r: S \times A \to \mathbb{R}$
 - a collection of probability distributions $p(\cdot | i, a)$, one for each $(i, a) \in S \times A$.

The idea is that there is a system and a decision maker. At each time instant, the decision maker observes the state of the system and chooses an action. The precise setup depends on what kind of problem it is:

 - in a **finite horizon** MDP, $T = \{0, 1, \ldots, N\}$, so the decision maker observes the state at each time $t \in T$ and chooses an action. After the last decision at time $N$, the system will transition to some state at time $N+1$ and stop.

 - in an **infinite horizon** MDP, $T = \{0, 1, \ldots\}$, and we just have an infinite sequence of

     (state) -> (choose action) -> (state transition) -> (choose action) -> ...

At each decision instant, supposing the current state is $i$ and upon choosing the action $a$, the decision maker obtains a reward $r(i, a)$, and the system transitions to its next state according to the probability $p(\cdot | i, a)$.

A decision maker's actions are chosen according to the **policy** it uses. Most broadly, a policy is any way of choosing actions at each decision instant. For convenience we study a more narrow class of policies as a first example. Formally, we can define a **policy** to be a finite or infinite sequence (depending on the horizon) $\pi = (d_t : t \in T)$, where each $d_t$ is sometimes called a **decision rule**.  A decision rule is either a **pure** decision rule:

$$d_t: S \to A$$

or a **randomized** decision rule:

$$d_t: S \to \{\text{all probability distributions on } A\}$$

When a policy consists entirely of pure/randomized decision rules, it is said to be a pure/randomized policy.

(See Puterman for a generalization of this definition. The above definition of policy corresponds to "Markovian policy" in Puterman's terminology.)

A **stationary** policy is one where a single decision rule is used at every decision instant.
