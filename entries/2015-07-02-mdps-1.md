---
title: Markov decision processes
---

The definition of Markov decision process given below is not the broadest possible definition, but I admit to being overwhelmed by more general definitions at this point so I'll stick with this for now in order to have something simple to grasp.

A **finite Markov decision process** includes the following mathematical objects:

 - a set of decision instants $T$, which we specify in more detail below
 - a finite state space $S$
 - a finite action space $A$
 - a reward function $r: S \times A \to \mathbb{R}$
 - a collection of probability distributions $p(\cdot | i, a)$, one for each $(i, a) \in S \times A$.

The idea is that there is a system and a decision maker. At each time instant, the decision maker observes the state of the system and chooses an action. The precise setup depends on what kind of problem it is:

 - in a **finite horizon** MDP, $T = \{0, 1, \ldots, N-1\}$, so the decision maker observes the state at each time $t \in T$ and chooses an action. After the last decision at time $N-1$, the system will transition to some state at time $N$ (the *final state*) and stop.

 - in an **infinite horizon** MDP, $T = \{0, 1, \ldots\}$, and we just have an infinite sequence of

     (initial state) -> (choose action) -> (state transition) -> (choose action) -> ...

At each decision instant, supposing the current state is $i$ and upon choosing the action $a$, the decision maker obtains a reward $r(i, a)$, and the system transitions to its next state according to the probability $p(\cdot | i, a)$.

A decision maker's actions are chosen according to the **policy** it uses. Most generally, a policy is any way of choosing actions at each decision instant. For convenience we study a more narrow class of policies and define a **policy** to be a finite or infinite sequence (depending on the horizon) $\pi = (d_t : t \in T)$, where each $d_t$ is a **decision rule**.  A decision rule is either a **pure** decision rule:

$$d_t: S \to A$$

or a **randomized** decision rule:

$$d_t: S \to \{\text{all probability distributions on } A\}$$

Pure decision rules directly tell you which action to take given the current state. Randomized decision rules, on the other hand, are used by the decision maker to choose actions randomly: if the system is in state $s$ at time $t$, the action is chosen randomly according to the distribution $d_t(s)$.

When a policy consists entirely of pure (randomized) decision rules, it is said to be a **pure** (**randomized**) policy.

(See Puterman for a generalization of this definition. The above definition of policy corresponds to "Markovian policy" in Puterman's terminology.)

A **stationary** policy is one where a single decision rule is used at every decision instant.
