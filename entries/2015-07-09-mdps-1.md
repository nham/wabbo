---
title: Markov decision processes
---

The definition of Markov decision process given below is not the broadest possible definition, but I admit to being overwhelmed by more general definitions at this point so I'll stick with this for now in order to have something simple to grasp.

A **finite Markov decision process** includes the following mathematical objects:

 - a set of decision instants $T$, which we specify in more detail below
 - a finite state space $S$
 - a finite action space $A$
 - a reward function $r: S \times A \to \mathbb{R}$
 - a collection of probability distributions $p(\cdot | i, a)$, one for each $(i, a) \in S \times A$.

The idea is that there is a system and a decision maker. At each time instant, the decision maker observes the state of the system and chooses an action. The precise setup depends on what kind of problem it is:

 - in a **finite horizon** MDP, $T = \{0, 1, \ldots, N-1\}$, so the decision maker observes the state at each time $t \in T$ and chooses an action. After the last decision at time $N-1$, the system will transition to some state at time $N$ (the *final state*), and the process ends. In addition to the reward function $r$, we need a **final reward function** $r_N: S \to \mathbb{R}$, so that $r_N(s)$ is the reward from ending the process in state $s$.

 - in an **infinite horizon** MDP, $T = \{0, 1, \ldots\}$, and we just have an infinite sequence of

     (initial state) -> (choose action) -> (state transition) -> (choose action) -> ...

At each decision instant, supposing the current state is $i$ and upon choosing the action $a$, the decision maker obtains a reward $r(i, a)$, and the system transitions to its next state according to the probability $p(\cdot | i, a)$.

A decision maker's actions are chosen according to the **policy** it uses. Most generally, a policy is any way of choosing actions at each decision instant. For convenience we study a more narrow class of policies and define a **policy** to be a finite or infinite sequence (depending on the horizon) $\pi = (d_t : t \in T)$, where each $d_t$ is a **decision rule**.  A decision rule is either a **pure** decision rule:

$$d_t: S \to A$$

or a **randomized** decision rule:

$$d_t: S \to \{\text{all probability distributions on } A\}$$

Pure decision rules directly tell you which action to take given the current state. Randomized decision rules, on the other hand, are used by the decision maker to choose actions randomly: if the system is in state $s$ at time $t$, the action is chosen randomly according to the distribution $d_t(s)$.

It should be noted that pure decision rules can be considered to be randomized decision rules with degenerate probability distributions (i.e. each $d_t(s)$ is a distribution with a single certain outcome).

When a policy consists entirely of pure (randomized) decision rules, it is said to be a **pure** (**randomized**) policy.

(See Puterman for a generalization of this definition. The above definition of policy corresponds to "Markovian policy" in Puterman's terminology.)

A **stationary** policy is one where a single decision rule is used at every decision instant.

## Utility functions

A **utility function** for a Markov decision process is a map

$$(\text{starting state}, \text{policy}) \mapsto \text{utility}$$

A few examples of utility functions are:

 - total expected reward over a finite horizon
 - total expected discounted reward over an infinite horizon
 - average expected reward over an infinite horizon

## Finite horizon problems

To go further it seems easiest to have separate analysis of finite and infinite horizon problems.

Suppose we have some finite horizon, finite MDP. We can define a probability space on $\Omega = (S \times A)^N \times S$. A trial in this space will sometimes be called a **path**, $\omega = (s_0, a_0, \ldots, s_{N-1}, a_{N-1}, s_N)$. We can then define random variables $X_t$ and $A_t$ for all $t$ by

$$X_t(\omega) := s_t$$

and

$$A_t(\omega) := a_t$$

These random variables describe the state and the action, respectively, at time $t$. Note that they are discrete since $S$ and $A$ are both finite (and would remain discrete even if we generalized $S$ and $A$ to be countable).

We can relate these to the transition distributions $p(\cdot | i, a)$ that are part of the basic data of any Markov decision process in the following way:

$$\mathbb{P}(X_{t+1} = j | X_t = i, A_t = a) = p(j | i, a)$$

for any $t$.

TODO: Actually, doesn't any probability involving $X_t, A_t$ need to be parameterized by the policy? $\mathbb{P}_{\pi}(\ldots)$ instead of $\mathbb{P}(\ldots)$ ?

Another useful collection of random variables we can define is $R_t$, for the reward at time $t$:

$$R_t(\omega) := r(s_t, a_t)$$

The **total reward** from a finite horizon MDP is then
